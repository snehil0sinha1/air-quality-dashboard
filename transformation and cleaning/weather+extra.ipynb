{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9938b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AQI Calculation\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12d0ed0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] Failed to open local file 'C:/Users/SNEHIL/Downloads/Air quality Monitoring/air_quality_with_weather.parquet'. Detail: [Windows error 2] The system cannot find the file specified.\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[0;32m      3\u001b[0m input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/SNEHIL/Downloads/Air quality Monitoring/air_quality_with_weather.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# your local path\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m pf \u001b[38;5;241m=\u001b[39m \u001b[43mpq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParquetFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_row_groups:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pf\u001b[38;5;241m.\u001b[39mnum_row_groups)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParquet schema (arrow):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SNEHIL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\parquet\\core.py:324\u001b[0m, in \u001b[0;36mParquetFile.__init__\u001b[1;34m(self, source, metadata, common_metadata, read_dictionary, binary_type, list_type, memory_map, buffer_size, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, filesystem, page_checksum_verification, arrow_extensions_enabled)\u001b[0m\n\u001b[0;32m    321\u001b[0m filesystem, source \u001b[38;5;241m=\u001b[39m _resolve_filesystem_and_path(\n\u001b[0;32m    322\u001b[0m     source, filesystem, memory_map\u001b[38;5;241m=\u001b[39mmemory_map)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filesystem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_input_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# We opened it here, ensure we close it.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader \u001b[38;5;241m=\u001b[39m ParquetReader()\n",
      "File \u001b[1;32mc:\\Users\\SNEHIL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\_fs.pyx:814\u001b[0m, in \u001b[0;36mpyarrow._fs.FileSystem.open_input_file\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\SNEHIL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\SNEHIL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Failed to open local file 'C:/Users/SNEHIL/Downloads/Air quality Monitoring/air_quality_with_weather.parquet'. Detail: [Windows error 2] The system cannot find the file specified.\r\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "input_path = \"C:/Users/SNEHIL/Downloads/Air quality Monitoring/air_quality_with_weather.parquet\"  # your local path\n",
    "\n",
    "pf = pq.ParquetFile(input_path)\n",
    "print(\"num_row_groups:\", pf.num_row_groups)\n",
    "print(\"Parquet schema (arrow):\")\n",
    "print(pf.schema_arrow)    # shows arrow types for each column\n",
    "\n",
    "# print each column name + type\n",
    "for field in pf.schema_arrow:\n",
    "    print(field.name, \"->\", field.type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac70f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row groups: 15\n",
      "Processing row group 1/15 ...\n",
      "Processing row group 2/15 ...\n",
      "Processing row group 3/15 ...\n",
      "Processing row group 4/15 ...\n",
      "Processing row group 5/15 ...\n",
      "Processing row group 6/15 ...\n",
      "Processing row group 7/15 ...\n",
      "Processing row group 8/15 ...\n",
      "Processing row group 9/15 ...\n",
      "Processing row group 10/15 ...\n",
      "Processing row group 11/15 ...\n",
      "Processing row group 12/15 ...\n",
      "Processing row group 13/15 ...\n",
      "Processing row group 14/15 ...\n",
      "Processing row group 15/15 ...\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from pathlib import Path\n",
    "\n",
    "input_path = \"C:/Users/SNEHIL/Downloads/Air quality Monitoring/air_quality_with_weather.parquet\"\n",
    "output_path = \"C:/Users/SNEHIL/Downloads/Air quality Monitoring/fixed.parquet\"\n",
    "\n",
    "reader = pq.ParquetFile(input_path)\n",
    "print(\"Row groups:\", reader.num_row_groups)\n",
    "\n",
    "writer = None\n",
    "try:\n",
    "    for rg in range(reader.num_row_groups):\n",
    "        print(f\"Processing row group {rg+1}/{reader.num_row_groups} ...\")\n",
    "        table = reader.read_row_group(rg)  # pyarrow.Table for that row group\n",
    "\n",
    "        # On first row-group, create writer with the table's schema.\n",
    "        # coerce_timestamps='ms' will cast INT96/nanos to milliseconds.\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(\n",
    "                output_path,\n",
    "                table.schema,\n",
    "                coerce_timestamps='ms',              # cast timestamps to ms\n",
    "                allow_truncated_timestamps=True      # allow truncation if needed\n",
    "            )\n",
    "\n",
    "        writer.write_table(table)\n",
    "\n",
    "finally:\n",
    "    if writer is not None:\n",
    "        writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834eedb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"C:/Users/SNEHIL/Downloads/Air quality Monitoring/fixed.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339360c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location_id: long (nullable = true)\n",
      " |-- sensors_id: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- datetime: timestamp_ntz (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- value_co: double (nullable = true)\n",
      " |-- value_no2: double (nullable = true)\n",
      " |-- value_o3: double (nullable = true)\n",
      " |-- value_pm10: double (nullable = true)\n",
      " |-- value_pm25: double (nullable = true)\n",
      " |-- value_so2: double (nullable = true)\n",
      " |-- datetime_hour: timestamp_ntz (nullable = true)\n",
      " |-- lat_round: double (nullable = true)\n",
      " |-- lon_round: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- wind: double (nullable = true)\n",
      " |-- humidity: long (nullable = true)\n",
      " |-- units: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eee47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoints = {\n",
    "    \"pm25\": [(0, 30, 0, 50), (31, 60, 51, 100), (61, 90, 101, 200), (91, 120, 201, 300), (121, 250, 301, 400), (251, 9999, 401, 500)],\n",
    "    \"pm10\": [(0, 50, 0, 50), (51, 100, 51, 100), (101, 250, 101, 200), (251, 350, 201, 300), (351, 430, 301, 400), (431, 9999, 401, 500)],\n",
    "    \"no2\":  [(0, 40, 0, 50), (41, 80, 51, 100), (81, 180, 101, 200), (181, 280, 201, 300), (281, 400, 301, 400), (401, 9999, 401, 500)],\n",
    "    \"so2\":  [(0, 40, 0, 50), (41, 80, 51, 100), (81, 380, 101, 200), (381, 800, 201, 300), (801, 1600, 301, 400), (1601, 9999, 401, 500)],\n",
    "    \"co\":   [(0, 1, 0, 50), (1.1, 2, 51, 100), (2.1, 10, 101, 200), (10.1, 17, 201, 300), (17.1, 34, 301, 400), (34.1, 9999, 401, 500)],\n",
    "    \"o3\":   [(0, 50, 0, 50), (51, 100, 51, 100), (101, 168, 101, 200), (169, 208, 201, 300), (209, 748, 301, 400), (749, 9999, 401, 500)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5095b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def compute_aqi(cp, pollutant):\n",
    "    if cp is None:\n",
    "        return None\n",
    "    for bp_lo, bp_hi, i_lo, i_hi in breakpoints[pollutant]:\n",
    "        if bp_lo <= cp <= bp_hi:\n",
    "            return ((i_hi - i_lo) / (bp_hi - bp_lo)) * (cp - bp_lo) + i_lo\n",
    "    return None\n",
    "\n",
    "# Register UDFs\n",
    "for pol in breakpoints.keys():\n",
    "    spark.udf.register(f\"aqi_{pol}\", lambda x, pol=pol: compute_aqi(x, pol), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a18ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea25f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, greatest\n",
    "\n",
    "df = df.withColumn(\"aqi_pm25\",  udf(lambda x: compute_aqi(x, \"pm25\"), DoubleType())(col(\"value_pm25\")))\n",
    "df = df.withColumn(\"aqi_pm10\",  udf(lambda x: compute_aqi(x, \"pm10\"), DoubleType())(col(\"value_pm10\")))\n",
    "df = df.withColumn(\"aqi_no2\",   udf(lambda x: compute_aqi(x, \"no2\"), DoubleType())(col(\"value_no2\")))\n",
    "df = df.withColumn(\"aqi_so2\",   udf(lambda x: compute_aqi(x, \"so2\"), DoubleType())(col(\"value_so2\")))\n",
    "df = df.withColumn(\"aqi_co\",    udf(lambda x: compute_aqi(x, \"co\"), DoubleType())(col(\"value_co\")))\n",
    "df = df.withColumn(\"aqi_o3\",    udf(lambda x: compute_aqi(x, \"o3\"), DoubleType())(col(\"value_o3\")))\n",
    "\n",
    "# Final AQI\n",
    "df = df.withColumn(\"AQI\", greatest(\"aqi_pm25\", \"aqi_pm10\", \"aqi_no2\", \"aqi_so2\", \"aqi_co\", \"aqi_o3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01a644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location_id: long (nullable = true)\n",
      " |-- sensors_id: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- datetime: timestamp_ntz (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- value_co: double (nullable = true)\n",
      " |-- value_no2: double (nullable = true)\n",
      " |-- value_o3: double (nullable = true)\n",
      " |-- value_pm10: double (nullable = true)\n",
      " |-- value_pm25: double (nullable = true)\n",
      " |-- value_so2: double (nullable = true)\n",
      " |-- datetime_hour: timestamp_ntz (nullable = true)\n",
      " |-- lat_round: double (nullable = true)\n",
      " |-- lon_round: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- wind: double (nullable = true)\n",
      " |-- humidity: long (nullable = true)\n",
      " |-- units: string (nullable = true)\n",
      " |-- aqi_pm25: double (nullable = true)\n",
      " |-- aqi_pm10: double (nullable = true)\n",
      " |-- aqi_no2: double (nullable = true)\n",
      " |-- aqi_so2: double (nullable = true)\n",
      " |-- aqi_co: double (nullable = true)\n",
      " |-- aqi_o3: double (nullable = true)\n",
      " |-- AQI: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f85f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.1\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5377781",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.5.1 findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c7ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"C:/Users/SNEHIL/Downloads/Air quality Monitoring/fixed.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2603ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>sensors_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>value_pm10</th>\n",
       "      <th>value_pm25</th>\n",
       "      <th>value_so2</th>\n",
       "      <th>datetime_hour</th>\n",
       "      <th>lat_round</th>\n",
       "      <th>lon_round</th>\n",
       "      <th>temperature</th>\n",
       "      <th>wind</th>\n",
       "      <th>humidity</th>\n",
       "      <th>units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>SPARTAN - IIT Kanpur-12</td>\n",
       "      <td>2013-12-14 16:00:00</td>\n",
       "      <td>26.519</td>\n",
       "      <td>80.233</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-12-14 16:00:00</td>\n",
       "      <td>26.519</td>\n",
       "      <td>80.233</td>\n",
       "      <td>22.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>40</td>\n",
       "      <td>µg/m³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>SPARTAN - IIT Kanpur-12</td>\n",
       "      <td>2013-12-14 17:00:00</td>\n",
       "      <td>26.519</td>\n",
       "      <td>80.233</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>17:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-12-14 17:00:00</td>\n",
       "      <td>26.519</td>\n",
       "      <td>80.233</td>\n",
       "      <td>20.6</td>\n",
       "      <td>7.2</td>\n",
       "      <td>46</td>\n",
       "      <td>µg/m³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>SPARTAN - IIT Kanpur-12</td>\n",
       "      <td>2013-12-14 18:00:00</td>\n",
       "      <td>26.519</td>\n",
       "      <td>80.233</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-12-14 18:00:00</td>\n",
       "      <td>26.519</td>\n",
       "      <td>80.233</td>\n",
       "      <td>18.8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>52</td>\n",
       "      <td>µg/m³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>SPARTAN - IIT Kanpur-12</td>\n",
       "      <td>2013-12-14 19:00:00</td>\n",
       "      <td>26.519</td>\n",
       "      <td>80.233</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-12-14 19:00:00</td>\n",
       "      <td>26.519</td>\n",
       "      <td>80.233</td>\n",
       "      <td>17.2</td>\n",
       "      <td>9.4</td>\n",
       "      <td>59</td>\n",
       "      <td>µg/m³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>SPARTAN - IIT Kanpur-12</td>\n",
       "      <td>2013-12-14 20:00:00</td>\n",
       "      <td>26.519</td>\n",
       "      <td>80.233</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-12-14 20:00:00</td>\n",
       "      <td>26.519</td>\n",
       "      <td>80.233</td>\n",
       "      <td>15.8</td>\n",
       "      <td>8.2</td>\n",
       "      <td>65</td>\n",
       "      <td>µg/m³</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   location_id  sensors_id                 location            datetime  \\\n",
       "0           12          23  SPARTAN - IIT Kanpur-12 2013-12-14 16:00:00   \n",
       "1           12          23  SPARTAN - IIT Kanpur-12 2013-12-14 17:00:00   \n",
       "2           12          23  SPARTAN - IIT Kanpur-12 2013-12-14 18:00:00   \n",
       "3           12          23  SPARTAN - IIT Kanpur-12 2013-12-14 19:00:00   \n",
       "4           12          23  SPARTAN - IIT Kanpur-12 2013-12-14 20:00:00   \n",
       "\n",
       "      lat     lon  year  month  day      time  ...  value_pm10  value_pm25  \\\n",
       "0  26.519  80.233  2013     12   14  16:00:00  ...         0.0       106.5   \n",
       "1  26.519  80.233  2013     12   14  17:00:00  ...         0.0       127.6   \n",
       "2  26.519  80.233  2013     12   14  18:00:00  ...         0.0       124.0   \n",
       "3  26.519  80.233  2013     12   14  19:00:00  ...         0.0        84.9   \n",
       "4  26.519  80.233  2013     12   14  20:00:00  ...         0.0        36.8   \n",
       "\n",
       "   value_so2       datetime_hour  lat_round  lon_round temperature  wind  \\\n",
       "0        0.0 2013-12-14 16:00:00     26.519     80.233        22.2   3.8   \n",
       "1        0.0 2013-12-14 17:00:00     26.519     80.233        20.6   7.2   \n",
       "2        0.0 2013-12-14 18:00:00     26.519     80.233        18.8   9.0   \n",
       "3        0.0 2013-12-14 19:00:00     26.519     80.233        17.2   9.4   \n",
       "4        0.0 2013-12-14 20:00:00     26.519     80.233        15.8   8.2   \n",
       "\n",
       "   humidity  units  \n",
       "0        40  µg/m³  \n",
       "1        46  µg/m³  \n",
       "2        52  µg/m³  \n",
       "3        59  µg/m³  \n",
       "4        65  µg/m³  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084564c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Breakpoints Dictionary (Remains the same) ---\n",
    "breakpoints = {\n",
    "    \"pm25\": [(0, 30, 0, 50), (31, 60, 51, 100), (61, 90, 101, 200), (91, 120, 201, 300), (121, 250, 301, 400), (251, 9999, 401, 500)],\n",
    "    \"pm10\": [(0, 50, 0, 50), (51, 100, 51, 100), (101, 250, 101, 200), (251, 350, 201, 300), (351, 430, 301, 400), (431, 9999, 401, 500)],\n",
    "    \"no2\":  [(0, 40, 0, 50), (41, 80, 51, 100), (81, 180, 101, 200), (181, 280, 201, 300), (281, 400, 301, 400), (401, 9999, 401, 500)],\n",
    "    \"so2\":  [(0, 40, 0, 50), (41, 80, 51, 100), (81, 380, 101, 200), (381, 800, 201, 300), (801, 1600, 301, 400), (1601, 9999, 401, 500)],\n",
    "    \"co\":   [(0, 1, 0, 50), (1.1, 2, 51, 100), (2.1, 10, 101, 200), (10.1, 17, 201, 300), (17.1, 34, 301, 400), (34.1, 9999, 401, 500)],\n",
    "    \"o3\":   [(0, 50, 0, 50), (51, 100, 51, 100), (101, 168, 101, 200), (169, 208, 201, 300), (209, 748, 301, 400), (749, 9999, 401, 500)]\n",
    "}\n",
    "\n",
    "# --- 2. The Core AQI Calculation Function (Remains the same) ---\n",
    "\n",
    "def compute_aqi(cp, pollutant):\n",
    "    \"\"\"\n",
    "    Computes the Air Quality Index (AQI) for a given pollutant concentration (Cp).\n",
    "    \"\"\"\n",
    "    if cp is None or pd.isna(cp):\n",
    "        return None\n",
    "    \n",
    "    # Ensure Cp is treated as a float for calculations\n",
    "    cp = float(cp)\n",
    "    \n",
    "    # Check for division by zero risk (Bphi - Bplo == 0)\n",
    "    for bp_lo, bp_hi, i_lo, i_hi in breakpoints[pollutant]:\n",
    "        if bp_hi == bp_lo:\n",
    "             continue # Skip invalid breakpoint ranges\n",
    "             \n",
    "        if bp_lo <= cp <= bp_hi:\n",
    "            # The AQI formula: I = [(Ihi - Ilo) / (Bphi - Bplo)] * (Cp - Bplo) + Ilo\n",
    "            return ((i_hi - i_lo) / (bp_hi - bp_lo)) * (cp - bp_lo) + i_lo\n",
    "            \n",
    "    return None\n",
    "\n",
    "\n",
    "# --- 3. Pandas Implementation (Replaces Spark Transformations) ---\n",
    "\n",
    "\n",
    "# List of pollutants and their corresponding DataFrame columns\n",
    "pollutants = {\n",
    "    \"pm25\": \"value_pm25\",\n",
    "    \"pm10\": \"value_pm10\",\n",
    "    \"no2\":  \"value_no2\",\n",
    "    \"so2\":  \"value_so2\",\n",
    "    \"co\":   \"value_co\",\n",
    "    \"o3\":   \"value_o3\"\n",
    "}\n",
    "\n",
    "# 3a. Calculate individual AQI columns using .apply()\n",
    "aqi_columns = []\n",
    "for pol, col_name in pollutants.items():\n",
    "    aqi_col_name = f\"aqi_{pol}\"\n",
    "    \n",
    "    # Use .apply() to execute the function row-wise on the concentration column.\n",
    "    # The 'lambda' handles passing the pollutant name for the breakpoints lookup.\n",
    "    df[aqi_col_name] = df[col_name].apply(lambda cp: compute_aqi(cp, pol))\n",
    "    \n",
    "    aqi_columns.append(aqi_col_name)\n",
    "\n",
    "# 3b. Final AQI calculation (Replaces Spark's 'greatest')\n",
    "# Use the .max(axis=1) method on the subset of AQI columns.\n",
    "# skipna=True ensures that if one AQI value is None/NaN, the max of the others is taken.\n",
    "df[\"AQI\"] = df[aqi_columns].max(axis=1, skipna=True)\n",
    "\n",
    "# print(df) # Uncomment to see the resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1fd134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>sensors_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>wind</th>\n",
       "      <th>humidity</th>\n",
       "      <th>units</th>\n",
       "      <th>aqi_pm25</th>\n",
       "      <th>aqi_pm10</th>\n",
       "      <th>aqi_no2</th>\n",
       "      <th>aqi_so2</th>\n",
       "      <th>aqi_co</th>\n",
       "      <th>aqi_o3</th>\n",
       "      <th>AQI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>SPARTAN - IIT Kanpur-12</td>\n",
       "      <td>2013-12-14 16:00:00</td>\n",
       "      <td>26.519</td>\n",
       "      <td>80.233</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>40</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>253.913793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>253.913793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>SPARTAN - IIT Kanpur-12</td>\n",
       "      <td>2013-12-14 17:00:00</td>\n",
       "      <td>26.519</td>\n",
       "      <td>80.233</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>17:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.2</td>\n",
       "      <td>46</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>306.065116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>306.065116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>SPARTAN - IIT Kanpur-12</td>\n",
       "      <td>2013-12-14 18:00:00</td>\n",
       "      <td>26.519</td>\n",
       "      <td>80.233</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>52</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>303.302326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>303.302326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>SPARTAN - IIT Kanpur-12</td>\n",
       "      <td>2013-12-14 19:00:00</td>\n",
       "      <td>26.519</td>\n",
       "      <td>80.233</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>9.4</td>\n",
       "      <td>59</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>182.589655</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.589655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>SPARTAN - IIT Kanpur-12</td>\n",
       "      <td>2013-12-14 20:00:00</td>\n",
       "      <td>26.519</td>\n",
       "      <td>80.233</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>65</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>60.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   location_id  sensors_id                 location            datetime  \\\n",
       "0           12          23  SPARTAN - IIT Kanpur-12 2013-12-14 16:00:00   \n",
       "1           12          23  SPARTAN - IIT Kanpur-12 2013-12-14 17:00:00   \n",
       "2           12          23  SPARTAN - IIT Kanpur-12 2013-12-14 18:00:00   \n",
       "3           12          23  SPARTAN - IIT Kanpur-12 2013-12-14 19:00:00   \n",
       "4           12          23  SPARTAN - IIT Kanpur-12 2013-12-14 20:00:00   \n",
       "\n",
       "      lat     lon  year  month  day      time  ...  wind  humidity  units  \\\n",
       "0  26.519  80.233  2013     12   14  16:00:00  ...   3.8        40  µg/m³   \n",
       "1  26.519  80.233  2013     12   14  17:00:00  ...   7.2        46  µg/m³   \n",
       "2  26.519  80.233  2013     12   14  18:00:00  ...   9.0        52  µg/m³   \n",
       "3  26.519  80.233  2013     12   14  19:00:00  ...   9.4        59  µg/m³   \n",
       "4  26.519  80.233  2013     12   14  20:00:00  ...   8.2        65  µg/m³   \n",
       "\n",
       "     aqi_pm25  aqi_pm10  aqi_no2 aqi_so2  aqi_co  aqi_o3         AQI  \n",
       "0  253.913793       0.0      0.0     0.0     0.0     0.0  253.913793  \n",
       "1  306.065116       0.0      0.0     0.0     0.0     0.0  306.065116  \n",
       "2  303.302326       0.0      0.0     0.0     0.0     0.0  303.302326  \n",
       "3  182.589655       0.0      0.0     0.0     0.0     0.0  182.589655  \n",
       "4   60.800000       0.0      0.0     0.0     0.0     0.0   60.800000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5278c",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.33 GiB for an array with shape (12, 14841314) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# List the columns you want to remove\u001b[39;00m\n\u001b[0;32m      2\u001b[0m columns_to_drop \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maqi_pm25\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maqi_pm10\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maqi_o3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m ]\n\u001b[1;32m---> 11\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns_to_drop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndividual AQI columns successfully dropped from df.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SNEHIL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5421\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5422\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5432\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5433\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5566\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5570\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5575\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SNEHIL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:4785\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4783\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4785\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4788\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\SNEHIL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:4866\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4863\u001b[0m     new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   4865\u001b[0m bm_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m axis_num \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 4866\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4867\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4868\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4869\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4872\u001b[0m \u001b[43m    \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4873\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4874\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_mgr, axes\u001b[38;5;241m=\u001b[39mnew_mgr\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   4875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\SNEHIL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\SNEHIL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:843\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[0;32m    841\u001b[0m                     blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    842\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 843\u001b[0m                 nb \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m                 blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "File \u001b[1;32mc:\\Users\\SNEHIL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SNEHIL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SNEHIL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    155\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    162\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.33 GiB for an array with shape (12, 14841314) and data type float64"
     ]
    }
   ],
   "source": [
    "# List the columns you want to remove\n",
    "columns_to_drop = [\n",
    "    'aqi_pm25', \n",
    "    'aqi_pm10', \n",
    "    'aqi_no2', \n",
    "    'aqi_so2', \n",
    "    'aqi_co', \n",
    "    'aqi_o3'\n",
    "]\n",
    "\n",
    "df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(\"Individual AQI columns successfully dropped from df.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ad221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame successfully written to Parquet file:\n",
      "C:\\Users\\SNEHIL\\Downloads\\Air quality Monitoring\\air_quality_with_weather.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = r\"C:\\Users\\SNEHIL\\Downloads\\Air quality Monitoring\\air_quality_with_weather1.parquet\"\n",
    "\n",
    "# --- Recommended Practice: Ensure the directory exists ---\n",
    "# This step prevents an error if the 'Air quality Monitoring' folder doesn't exist yet.\n",
    "output_dir = os.path.dirname(file_path)\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created directory: {output_dir}\")\n",
    "\n",
    "# Write the DataFrame to the Parquet file\n",
    "try:\n",
    "    df.to_parquet(\n",
    "        path=file_path, \n",
    "        engine='pyarrow',      # Highly recommended engine for Parquet\n",
    "        compression='snappy',  # A fast, efficient compression codec\n",
    "        index=False            # Set to False unless you specifically need to save the index\n",
    "    )\n",
    "    print(f\"\\nDataFrame successfully written to Parquet file:\\n{file_path}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nERROR: The 'pyarrow' or 'fastparquet' library is required to write Parquet files.\")\n",
    "    print(\"Please install one of them: `pip install pyarrow`\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while writing the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a5e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000020B4E2E1610>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSV Loader\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cfda81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------------+--------------+------+\n",
      "|StationId|         StationName|             City|         State|Status|\n",
      "+---------+--------------------+-----------------+--------------+------+\n",
      "|    AP001|Secretariat, Amar...|        Amaravati|Andhra Pradesh|Active|\n",
      "|    AP002|Anand Kala Kshetr...|Rajamahendravaram|Andhra Pradesh|  NULL|\n",
      "|    AP003|Tirumala, Tirupat...|         Tirupati|Andhra Pradesh|  NULL|\n",
      "|    AP004|PWD Grounds, Vija...|       Vijayawada|Andhra Pradesh|  NULL|\n",
      "|    AP005|GVM Corporation, ...|    Visakhapatnam|Andhra Pradesh|Active|\n",
      "|    AS001|Railway Colony, G...|         Guwahati|         Assam|Active|\n",
      "|    BR001|Collectorate, Gay...|             Gaya|         Bihar|  NULL|\n",
      "|    BR002|SFTI Kusdihra, Ga...|             Gaya|         Bihar|  NULL|\n",
      "|    BR003|Industrial Area, ...|          Hajipur|         Bihar|  NULL|\n",
      "|    BR004|Muzaffarpur Colle...|      Muzaffarpur|         Bihar|  NULL|\n",
      "|    BR005|DRM Office Danapu...|            Patna|         Bihar|Active|\n",
      "|    BR006|Govt. High School...|            Patna|         Bihar|Active|\n",
      "|    BR007|IGSC Planetarium ...|            Patna|         Bihar|Active|\n",
      "|    BR008|Muradpur, Patna -...|            Patna|         Bihar|Active|\n",
      "|    BR009|Rajbansi Nagar, P...|            Patna|         Bihar|Active|\n",
      "|    BR010|Samanpura, Patna ...|            Patna|         Bihar|Active|\n",
      "|    CH001|Sector-25, Chandi...|       Chandigarh|    Chandigarh|Active|\n",
      "|    DL001|Alipur, Delhi - DPCC|            Delhi|         Delhi|Active|\n",
      "|    DL002|Anand Vihar, Delh...|            Delhi|         Delhi|Active|\n",
      "|    DL003|Ashok Vihar, Delh...|            Delhi|         Delhi|Active|\n",
      "+---------+--------------------+-----------------+--------------+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "stations_df = spark.read.csv(\"./stations.csv\", header=True, inferSchema=True)\n",
    "stations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af71a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------------+--------------+------+\n",
      "|StationId|         StationName|             City|         State|Status|\n",
      "+---------+--------------------+-----------------+--------------+------+\n",
      "|    AP001|Secretariat, Amar...|        Amaravati|Andhra Pradesh|Active|\n",
      "|    AP002|Anand Kala Kshetr...|Rajamahendravaram|Andhra Pradesh|  NULL|\n",
      "|    AP003| Tirumala, Tirupati |         Tirupati|Andhra Pradesh|  NULL|\n",
      "|    AP004|PWD Grounds, Vija...|       Vijayawada|Andhra Pradesh|  NULL|\n",
      "|    AP005|GVM Corporation, ...|    Visakhapatnam|Andhra Pradesh|Active|\n",
      "+---------+--------------------+-----------------+--------------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "stations_df = stations_df.withColumn(\"StationName\", regexp_replace(\"StationName\", \"-.*\", \"\"))\n",
    "stations_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aabff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stations_df = pd.read_csv('./stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4449f3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationId</th>\n",
       "      <th>StationName</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP001</td>\n",
       "      <td>Secretariat, Amaravati - APPCB</td>\n",
       "      <td>Amaravati</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>Active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP002</td>\n",
       "      <td>Anand Kala Kshetram, Rajamahendravaram - APPCB</td>\n",
       "      <td>Rajamahendravaram</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP003</td>\n",
       "      <td>Tirumala, Tirupati - APPCB</td>\n",
       "      <td>Tirupati</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP004</td>\n",
       "      <td>PWD Grounds, Vijayawada - APPCB</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP005</td>\n",
       "      <td>GVM Corporation, Visakhapatnam - APPCB</td>\n",
       "      <td>Visakhapatnam</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>Active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StationId                                     StationName  \\\n",
       "0     AP001                  Secretariat, Amaravati - APPCB   \n",
       "1     AP002  Anand Kala Kshetram, Rajamahendravaram - APPCB   \n",
       "2     AP003                      Tirumala, Tirupati - APPCB   \n",
       "3     AP004                 PWD Grounds, Vijayawada - APPCB   \n",
       "4     AP005          GVM Corporation, Visakhapatnam - APPCB   \n",
       "\n",
       "                City           State  Status  \n",
       "0          Amaravati  Andhra Pradesh  Active  \n",
       "1  Rajamahendravaram  Andhra Pradesh     NaN  \n",
       "2           Tirupati  Andhra Pradesh     NaN  \n",
       "3         Vijayawada  Andhra Pradesh     NaN  \n",
       "4      Visakhapatnam  Andhra Pradesh  Active  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f5906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  StationId                              StationName               City  \\\n",
      "0     AP001                  Secretariat, Amaravati           Amaravati   \n",
      "1     AP002  Anand Kala Kshetram, Rajamahendravaram   Rajamahendravaram   \n",
      "2     AP003                      Tirumala, Tirupati            Tirupati   \n",
      "3     AP004                 PWD Grounds, Vijayawada          Vijayawada   \n",
      "4     AP005          GVM Corporation, Visakhapatnam       Visakhapatnam   \n",
      "\n",
      "            State  Status  \n",
      "0  Andhra Pradesh  Active  \n",
      "1  Andhra Pradesh     NaN  \n",
      "2  Andhra Pradesh     NaN  \n",
      "3  Andhra Pradesh     NaN  \n",
      "4  Andhra Pradesh  Active  \n"
     ]
    }
   ],
   "source": [
    "stations_df[\"StationName\"] = stations_df[\"StationName\"].str.replace(\"-.*\", \"\", regex=True)\n",
    "print(stations_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dbe814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SNEHIL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting geocoding for Full Location...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [07:28<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting geocoding for City/State fallback on 94 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [03:43<00:00,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final DataFrame structure:\n",
      "  StationId                              StationName               City  \\\n",
      "0     AP001                  Secretariat, Amaravati           Amaravati   \n",
      "1     AP002  Anand Kala Kshetram, Rajamahendravaram   Rajamahendravaram   \n",
      "2     AP003                      Tirumala, Tirupati            Tirupati   \n",
      "3     AP004                 PWD Grounds, Vijayawada          Vijayawada   \n",
      "4     AP005          GVM Corporation, Visakhapatnam       Visakhapatnam   \n",
      "\n",
      "            State  Status  longitude   latitude  \n",
      "0  Andhra Pradesh  Active  80.485951  16.537386  \n",
      "1  Andhra Pradesh     NaN        NaN        NaN  \n",
      "2  Andhra Pradesh     NaN  79.349752  13.679524  \n",
      "3  Andhra Pradesh     NaN        NaN        NaN  \n",
      "4  Andhra Pradesh  Active        NaN        NaN  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 230 entries, 0 to 229\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   StationId    230 non-null    object \n",
      " 1   StationName  230 non-null    object \n",
      " 2   City         230 non-null    object \n",
      " 3   State        230 non-null    object \n",
      " 4   Status       133 non-null    object \n",
      " 5   longitude    136 non-null    float64\n",
      " 6   latitude     136 non-null    float64\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 12.7+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "# --- NEW IMPORT ---\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Enable tqdm progress bar for pandas operations\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- Setup for geocoding ---\n",
    "# Initialize the geolocator once outside the function for efficiency\n",
    "geolocator = Nominatim(user_agent=\"air_quality_app\")\n",
    "# Define the rate-limit delay\n",
    "DELAY_SECONDS = 2 # Increased delay to be safe with Nominatim\n",
    "\n",
    "def geocode_location_safe(location):\n",
    "    \"\"\"\n",
    "    Geocodes a location string and returns (longitude, latitude).\n",
    "    Includes a delay for rate limiting.\n",
    "    \"\"\"\n",
    "    if pd.isna(location) or location.strip() == \"\":\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        # NOTE: Using the globally defined geolocator\n",
    "        location_data = geolocator.geocode(location)\n",
    "        if location_data:\n",
    "            # Add a small delay to avoid hitting API rate limits\n",
    "            time.sleep(DELAY_SECONDS)\n",
    "            return (location_data.longitude, location_data.latitude)\n",
    "        return (None, None)\n",
    "    except Exception as e:\n",
    "        # Wait even if there's an error to respect rate limits\n",
    "        time.sleep(DELAY_SECONDS)\n",
    "        return (None, None)\n",
    "\n",
    "\n",
    "# --- Apply geocoding logic to the pandas DataFrame ---\n",
    "\n",
    "# 1. Create the full location string\n",
    "stations_df[\"City\"] = stations_df[\"City\"].fillna(\"\")\n",
    "stations_df[\"State\"] = stations_df[\"State\"].fillna(\"\")\n",
    "stations_df[\"location_full\"] = stations_df[\"StationName\"] + \", \" + stations_df[\"City\"] + \", \" + stations_df[\"State\"]\n",
    "\n",
    "# 2. Apply geocoding for the full location\n",
    "# --- ADDED PROGRESS BAR HERE (using .progress_apply) ---\n",
    "print(\"Starting geocoding for Full Location...\")\n",
    "stations_df[[\"longitude_full\", \"latitude_full\"]] = stations_df[\"location_full\"].progress_apply(\n",
    "    lambda x: geocode_location_safe(x)\n",
    ").apply(pd.Series)\n",
    "\n",
    "# 3. Create the City & State location string\n",
    "stations_df[\"location_city_state\"] = stations_df[\"City\"] + \", \" + stations_df[\"State\"]\n",
    "\n",
    "# 4. Apply geocoding for City & State, but ONLY where full geocoding failed (lon is NaN)\n",
    "failed_mask = stations_df[\"longitude_full\"].isna()\n",
    "rows_to_geocode = failed_mask.sum()\n",
    "\n",
    "print(f\"\\nStarting geocoding for City/State fallback on {rows_to_geocode} rows...\")\n",
    "\n",
    "# Only apply the function to the rows where the full geocoding failed\n",
    "# --- ADDED PROGRESS BAR HERE (using .progress_apply on the subset) ---\n",
    "stations_df.loc[failed_mask, [\"longitude_city_state\", \"latitude_city_state\"]] = stations_df.loc[failed_mask, \"location_city_state\"].progress_apply(\n",
    "    lambda x: geocode_location_safe(x)\n",
    ").apply(pd.Series)\n",
    "\n",
    "# Fill remaining non-failed rows with NaN to ensure consistent column structure\n",
    "stations_df[\"longitude_city_state\"] = stations_df[\"longitude_city_state\"].fillna(pd.NA)\n",
    "stations_df[\"latitude_city_state\"] = stations_df[\"latitude_city_state\"].fillna(pd.NA)\n",
    "\n",
    "\n",
    "# 5. Combine the results\n",
    "stations_df[\"longitude\"] = stations_df[\"longitude_full\"].combine_first(stations_df[\"longitude_city_state\"])\n",
    "stations_df[\"latitude\"] = stations_df[\"latitude_full\"].combine_first(stations_df[\"latitude_city_state\"])\n",
    "\n",
    "# 6. Drop the intermediate columns\n",
    "columns_to_drop = [\n",
    "    \"location_full\", \"longitude_full\", \"latitude_full\",\n",
    "    \"location_city_state\", \"longitude_city_state\", \"latitude_city_state\"\n",
    "]\n",
    "stations_df = stations_df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(\"\\nFinal DataFrame structure:\")\n",
    "print(stations_df.head(5))\n",
    "print(stations_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3948b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df['latitude'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf18e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df.to_csv('./stations_df1.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c0de19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000016F9A0FC470>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Weather Loader\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bbbea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+----------------------+------------------------------+--------------------+----+----+---------+------+----+-----+---+-------------------+----+------+------+---+------------+----------+----------+\n",
      "|_sampling_date|           state|city_town_village_area|location_of_monitoring_station|    type_of_location| so2| no2|rspm_pm10|pm_2_5|year|month|day|               time|hour|minute|second|aqi|aqi_category|  latitude| longitude|\n",
      "+--------------+----------------+----------------------+------------------------------+--------------------+----+----+---------+------+----+-----+---+-------------------+----+------+------+---+------------+----------+----------+\n",
      "|7/1/2014 12:48|       Meghalaya|                 Dawki|          Terrace building,...|Residential, Rura...| 2.0|11.0|     52.0|  NULL|2014|    7|  1|2025-10-03 12:48:04|  12|    48|     4| 52|Satisfactory|25.1856343|92.0215717|\n",
      "|1/1/2014 12:25|             Goa|                Panaji|          Infront of Old GS...|Residential, Rura...| 6.0| 9.0|     61.0|  NULL|2014|    1|  1|2025-10-03 12:25:16|  12|    25|    16| 61|Satisfactory|15.4989946|73.8282141|\n",
      "| 1/1/2010 4:37|      Chandigarh|            Chandigarh|          Modern Foods, Ind...|     Industrial Area| 2.0|36.0|     95.0|  NULL|2010|    1|  1|2025-10-03 04:37:47|   4|    37|    47| 95|Satisfactory|30.7334421|76.7797143|\n",
      "| 2/1/2004 4:41|           Assam|              Guwahati|          Head Office, Bamu...|Residential, Rura...| 2.0|16.2|    173.0|  NULL|2004|    2|  1|2025-10-03 04:41:29|   4|    41|    29|149|    Moderate|26.1805978| 91.753943|\n",
      "| 2/1/2014 4:56|     West Bengal|        South Suburban|          Amtala, South Sub...|     Industrial Area|13.0|75.0|    185.0|  NULL|2014|    2|  1|2025-10-03 04:56:23|   4|    56|    23|157|    Moderate|22.5097756|88.3214307|\n",
      "| 7/7/2014 4:58|      Puducherry|              Karaikal|          B.Ed College (PKC...|Residential, Rura...| 6.0| 9.0|     16.0|  NULL|2014|    7|  7|2025-10-03 04:58:12|   4|    58|    12| 16|        Good|  10.91571|79.8375761|\n",
      "| 2/1/2014 5:05|         Mizoram|                Aizawl|          Khatla, M.G-Road,...|Residential, Rura...| 2.0| 5.0|     53.0|  NULL|2014|    2|  1|2025-10-03 05:05:24|   5|     5|    24| 53|Satisfactory|23.7277631|92.7179947|\n",
      "| 1/1/2008 5:05|         Mizoram|                Aizawl|                     Bawngkawn|Residential and o...|NULL| 9.4|     60.0|  NULL|2008|    1|  1|2025-10-03 05:05:05|   5|     5|     5| 60|Satisfactory|23.7534536|   92.7281|\n",
      "| 1/1/2014 4:38|      Chandigarh|            Chandigarh|          Modern Foods, Ind...|     Industrial Area| 2.0|16.0|    247.0|  NULL|2014|    1|  1|2025-10-03 04:38:08|   4|    38|     8|198|    Moderate|30.7334421|76.7797143|\n",
      "| 6/1/2007 4:47|  Madhya Pradesh|                Bhopal|                    Govindpura|    Industrial Areas| 6.3|12.7|     69.0|  NULL|2007|    6|  1|2025-10-03 04:47:55|   4|    47|    55| 69|Satisfactory|23.2460198|77.4467044|\n",
      "| 2/1/2004 4:50|     Maharashtra|                Nashik|          RTO Colony Tank, ...|Residential, Rura...|42.0|28.0|     93.0|  NULL|2004|    2|  1|2025-10-03 04:50:31|   4|    50|    31| 93|Satisfactory|20.0112475|73.7902364|\n",
      "| 3/1/2012 5:01|          Kerala|             Alappuzha|          DC Mills Pathirap...|     Industrial Area| 2.0| 4.5|     60.0|  NULL|2012|    3|  1|2025-10-03 05:01:07|   5|     1|     7| 60|Satisfactory| 9.5006651|76.4124143|\n",
      "| 2/8/2005 5:05|        Nagaland|               Dimapur|                   Bank Colony|Residential and o...|NULL|NULL|     20.0|  NULL|2005|    2|  8|2025-10-03 05:05:47|   5|     5|    47| 20|        Good|25.9041378|93.7177609|\n",
      "| 3/1/2009 5:10|    Chhattisgarh|          Bhilai Nagar|                M.P.L.U. Nigam|    Industrial Areas|27.0|32.8|    162.0|  NULL|2009|    3|  1|2025-10-03 05:10:10|   5|    10|    10|142|    Moderate|21.2011836|81.3352266|\n",
      "| 1/1/2013 5:20|Himachal Pradesh|                 Baddi|                 Housing Board|Residential, Rura...| 3.0|20.0|    120.0|  NULL|2013|    1|  1|2025-10-03 05:20:17|   5|    20|    17|114|    Moderate|30.9763026|76.7674001|\n",
      "| 3/7/2009 6:57|  Andhra Pradesh|                Guntur|          Hindu College Guntur|Residential and o...| 4.0| 9.0|     79.0|  NULL|2009|    3|  7|2025-10-03 06:57:21|   6|    57|    21| 79|Satisfactory| 16.292349| 80.443957|\n",
      "| 3/1/2013 6:35|         Mizoram|                Aizawl|          Khatla, M.G-Road,...|Residential, Rura...| 2.0| 4.5|     49.0|  NULL|2013|    3|  1|2025-10-03 06:35:20|   6|    35|    20| 49|        Good|23.7277631|92.7179947|\n",
      "|2/1/2014 11:38|       Rajasthan|                 Alwar|          Regional Office, ...|Residential, Rura...| 8.0|18.0|    330.0|  NULL|2014|    2|  1|2025-10-03 11:38:39|  11|    38|    39|280|        Poor|27.4140048|76.6170387|\n",
      "| 3/1/2009 6:35|        Nagaland|               Dimapur|                   Bank Colony|Residential and o...|NULL|17.5|     62.0|  NULL|2009|    3|  1|2025-10-03 06:35:44|   6|    35|    44| 62|Satisfactory|25.9041378|93.7177609|\n",
      "| 4/4/2006 6:36|          Punjab|              Amritsar|           Nagina Soap Factory|    Industrial Areas|11.8|31.8|     NULL|  NULL|2006|    4|  4|2025-10-03 06:36:37|   6|    36|    37| 40|        Good|31.6356659|74.8787496|\n",
      "+--------------+----------------+----------------------+------------------------------+--------------------+----+----+---------+------+----+-----+---+-------------------+----+------+------+---+------------+----------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "historic_df = spark.read.csv(\"./transformation and cleaning/air_quality_final_20250929_105402.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with year less than 2000 and missing aqi values\n",
    "historic_df = historic_df.filter((historic_df.year >= 2000) & (historic_df.aqi.isNotNull()))\n",
    "\n",
    "# Drop the specified columns\n",
    "historic_df = historic_df.drop(\"_c21\", \"geocoding_source\")\n",
    "\n",
    "historic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8b69269",
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_df = spark.read.csv(\"./rest_missing.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c988e13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique (lat,lon,year) pairs: 193\n",
      "+----------+----------+----+\n",
      "|  latitude| longitude|year|\n",
      "+----------+----------+----+\n",
      "| 9.0014992|76.5347516|2011|\n",
      "|27.4844597|94.9019447|2015|\n",
      "|32.2504936|77.1881383|2014|\n",
      "|18.5004949|73.8529037|2004|\n",
      "|22.3474121|82.5340303|2009|\n",
      "+----------+----------+----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Get unique location-year pairs using Spark DataFrame operations\n",
    "unique_loc_year_spark = historic_df.select('latitude', 'longitude', 'year').distinct()\n",
    "\n",
    "# Show the count of unique location-year pairs\n",
    "print(\"Total unique (lat,lon,year) pairs:\", unique_loc_year_spark.count())\n",
    "\n",
    "# Display the first few rows of the unique location-year pairs\n",
    "unique_loc_year_spark.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb4f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tasks: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching weather: 100%|██████████| 193/193 [06:22<00:00,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No data fetched.\n",
      "⚠️ 193 requests failed. Errors saved to weather_fetch_errors.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, TimestampType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"WeatherFetcher\").getOrCreate()\n",
    "\n",
    "# Schema for final Spark DataFrame\n",
    "weather_schema = StructType([\n",
    "    StructField(\"datetime\", TimestampType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"wind\", DoubleType(), True),\n",
    "    StructField(\"humidity\", DoubleType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"lon\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# Function to fetch weather\n",
    "# ----------------------------\n",
    "def fetch_weather(lat, lon, year, max_retries=5):\n",
    "    lat = float(lat)\n",
    "    lon = float(lon)\n",
    "    year = int(year)\n",
    "\n",
    "    start = f\"{year}-01-01\"\n",
    "    end   = f\"{year}-12-31\"\n",
    "\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": start,\n",
    "        \"end_date\": end,\n",
    "        \"hourly\": [\"temperature_2m\", \"windspeed_10m\", \"relative_humidity_2m\"],\n",
    "        \"timezone\": \"Asia/Kolkata\"\n",
    "    }\n",
    "\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            r = requests.get(\"https://archive-api.open-meteo.com/v1/archive\", params=params, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "\n",
    "            # Case 1: API responds but has no data\n",
    "            if \"hourly\" not in data or not data[\"hourly\"].get(\"time\"):\n",
    "                return None, {\"lat\": lat, \"lon\": lon, \"year\": year, \"error\": \"no_data_available\"}\n",
    "\n",
    "            pdf = pd.DataFrame({\n",
    "                \"datetime\": pd.to_datetime(data[\"hourly\"][\"time\"]),\n",
    "                \"temperature\": data[\"hourly\"][\"temperature_2m\"],\n",
    "                \"wind\": data[\"hourly\"][\"windspeed_10m\"],\n",
    "                \"humidity\": data[\"hourly\"][\"relative_humidity_2m\"],\n",
    "                \"lat\": lat,\n",
    "                \"lon\": lon\n",
    "            })\n",
    "            return pdf, None  # ✅ success\n",
    "\n",
    "        except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n",
    "            retries += 1\n",
    "            if retries >= max_retries:\n",
    "                return None, {\"lat\": lat, \"lon\": lon, \"year\": year, \"error\": f\"api_failure: {str(e)}\"}\n",
    "            time.sleep(2 ** retries)\n",
    "        except Exception as e:\n",
    "            return None, {\"lat\": lat, \"lon\": lon, \"year\": year, \"error\": f\"unexpected: {str(e)}\"}\n",
    "\n",
    "    return None, {\"lat\": lat, \"lon\": lon, \"year\": year, \"error\": \"unknown_failure\"}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN PIPELINE\n",
    "# ----------------------------\n",
    "# Get your unique (lat, lon, year) pairs from Spark\n",
    "pairs = unique_loc_year_spark.collect()\n",
    "print(f\"Total tasks: {len(pairs)}\")\n",
    "\n",
    "results = []\n",
    "errors = []  # store failed requests\n",
    "\n",
    "# Run parallel fetching\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:  # adjust workers depending on CPU/internet\n",
    "    futures = {\n",
    "        executor.submit(fetch_weather, row.latitude, row.longitude, row.year): row \n",
    "        for row in pairs\n",
    "    }\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Fetching weather\"):\n",
    "        pdf, error = future.result()\n",
    "        if pdf is not None:\n",
    "            results.append(pdf)\n",
    "        if error is not None:\n",
    "            errors.append(error)\n",
    "\n",
    "# ----------------------------\n",
    "# Convert results to Spark DataFrame\n",
    "# ----------------------------\n",
    "if results:\n",
    "    all_weather_pd1 = pd.concat(results, ignore_index=True)\n",
    "    print(\"✅ Weather data saved successfully!\")\n",
    "else:\n",
    "    print(\"⚠️ No data fetched.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Save errors separately\n",
    "# ----------------------------\n",
    "if errors:\n",
    "    error_df = pd.DataFrame(errors)\n",
    "    print(f\"⚠️ {len(errors)} requests failed. Errors saved to weather_fetch_errors.parquet\")\n",
    "else:\n",
    "    print(\"✅ No errors encountered!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "79a51b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         lat        lon  year  \\\n",
      "0  18.500495  73.852904  2004   \n",
      "1  22.347412  82.534030  2009   \n",
      "2  21.149813  79.082056  2010   \n",
      "3  26.180598  91.753943  2010   \n",
      "4  26.296772  73.035143  2005   \n",
      "\n",
      "                                               error  \n",
      "0  429 Client Error: Too Many Requests for url: h...  \n",
      "1  429 Client Error: Too Many Requests for url: h...  \n",
      "2  429 Client Error: Too Many Requests for url: h...  \n",
      "3  429 Client Error: Too Many Requests for url: h...  \n",
      "4  429 Client Error: Too Many Requests for url: h...  \n"
     ]
    }
   ],
   "source": [
    "print(error_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379100b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = spark.createDataFrame(all_weather_pd, schema=weather_schema)\n",
    "\n",
    "    # Save fewer parquet files\n",
    "weather_df.coalesce(10).write.mode(\"overwrite\").parquet(\n",
    "        \"C:/Users/SNEHIL/Downloads/Air_quality_Monitoring/final_weather_data_all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "302859ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temperature</th>\n",
       "      <th>wind</th>\n",
       "      <th>humidity</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>11.2</td>\n",
       "      <td>7.9</td>\n",
       "      <td>81</td>\n",
       "      <td>26.400882</td>\n",
       "      <td>80.400005</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01 01:00:00</td>\n",
       "      <td>10.6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>82</td>\n",
       "      <td>26.400882</td>\n",
       "      <td>80.400005</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01 02:00:00</td>\n",
       "      <td>10.1</td>\n",
       "      <td>7.3</td>\n",
       "      <td>84</td>\n",
       "      <td>26.400882</td>\n",
       "      <td>80.400005</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01 03:00:00</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>85</td>\n",
       "      <td>26.400882</td>\n",
       "      <td>80.400005</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01 04:00:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>87</td>\n",
       "      <td>26.400882</td>\n",
       "      <td>80.400005</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  temperature  wind  humidity        lat        lon  year\n",
       "0 2013-01-01 00:00:00         11.2   7.9        81  26.400882  80.400005  2013\n",
       "1 2013-01-01 01:00:00         10.6   8.0        82  26.400882  80.400005  2013\n",
       "2 2013-01-01 02:00:00         10.1   7.3        84  26.400882  80.400005  2013\n",
       "3 2013-01-01 03:00:00          9.5   6.4        85  26.400882  80.400005  2013\n",
       "4 2013-01-01 04:00:00          9.0   5.9        87  26.400882  80.400005  2013"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weather_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8143fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'datetime' column is already in datetime format\n",
    "all_weather_pd['year'] = all_weather_pd['datetime'].dt.year \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ea8d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_all_weather_pd = all_weather_pd[['lat','lon','year']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ff8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_all_weather_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecae9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Perform a left anti join to find the non-matching combinations\n",
    "non_matching_combinations = unique_loc_year_spark.join(\n",
    "    uni_all_weather_spark,\n",
    "    on=(\n",
    "        (unique_loc_year_spark[\"latitude\"] == uni_all_weather_spark[\"lat\"]) &\n",
    "        (unique_loc_year_spark[\"longitude\"] == uni_all_weather_spark[\"lon\"]) &\n",
    "        (unique_loc_year_spark[\"year\"] == uni_all_weather_spark[\"year\"])\n",
    "    ),\n",
    "    how=\"left_anti\" # Use \"left_anti\" for this specific purpose\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac3e5b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2436600, 7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weather_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be14b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame with explicit options for better Spark compatibility\n",
    "# Use 'fastparquet' as the engine and specify the timestamp format\n",
    "# OR use pyarrow with specific version and format options\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet\n",
    "\n",
    "# Using pyarrow (recommended):\n",
    "df_dropped.to_parquet(\n",
    "    'all_weather_pd_compatible_final.parquet', \n",
    "    engine='pyarrow', \n",
    "    version='1.0',  # Use an older, more compatible Parquet version\n",
    "    allow_truncated_timestamps=True # Allows conversion from nanoseconds to microseconds/milliseconds\n",
    ")\n",
    "\n",
    "# Optional: Using fastparquet (sometimes resolves issues pyarrow can't):\n",
    "# all_weather_pd.to_parquet(\n",
    "#     'all_weather_pd_compatible.parquet', \n",
    "#     engine='fastparquet',\n",
    "#     times='int96' # This is an older, often-compatible timestamp format\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "852c3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weather_pd = pd.read_parquet(\"./all_weather_pd_compatible.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f11ffcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weather_pd1 = pd.read_parquet(\"./all_weather_pd_compatible1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66859949",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weather_pd2 = pd.read_parquet(\"./all_weather_pd_compatible2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7c3950f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temperature</th>\n",
       "      <th>wind</th>\n",
       "      <th>humidity</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>25.2</td>\n",
       "      <td>7.1</td>\n",
       "      <td>89</td>\n",
       "      <td>9.500665</td>\n",
       "      <td>76.412414</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01 01:00:00</td>\n",
       "      <td>25.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>89</td>\n",
       "      <td>9.500665</td>\n",
       "      <td>76.412414</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01 02:00:00</td>\n",
       "      <td>24.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>91</td>\n",
       "      <td>9.500665</td>\n",
       "      <td>76.412414</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01 03:00:00</td>\n",
       "      <td>24.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>93</td>\n",
       "      <td>9.500665</td>\n",
       "      <td>76.412414</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01 04:00:00</td>\n",
       "      <td>24.1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>94</td>\n",
       "      <td>9.500665</td>\n",
       "      <td>76.412414</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  temperature  wind  humidity       lat        lon  year\n",
       "0 2013-01-01 00:00:00         25.2   7.1        89  9.500665  76.412414   NaN\n",
       "1 2013-01-01 01:00:00         25.1   6.0        89  9.500665  76.412414   NaN\n",
       "2 2013-01-01 02:00:00         24.7   4.7        91  9.500665  76.412414   NaN\n",
       "3 2013-01-01 03:00:00         24.3   3.5        93  9.500665  76.412414   NaN\n",
       "4 2013-01-01 04:00:00         24.1   4.6        94  9.500665  76.412414   NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_weather_combined_pd = pd.concat([all_weather_pd,all_weather_pd1, all_weather_pd2], ignore_index=True)\n",
    "display(all_weather_combined_pd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "64149eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    datetime  temperature  wind  humidity        lat  \\\n",
      "0        2013-01-01 00:00:00         25.2   7.1        89   9.500665   \n",
      "1        2013-01-01 01:00:00         25.1   6.0        89   9.500665   \n",
      "2        2013-01-01 02:00:00         24.7   4.7        91   9.500665   \n",
      "3        2013-01-01 03:00:00         24.3   3.5        93   9.500665   \n",
      "4        2013-01-01 04:00:00         24.1   4.6        94   9.500665   \n",
      "...                      ...          ...   ...       ...        ...   \n",
      "21868459 2012-12-31 19:00:00         23.8   7.9        52  22.300123   \n",
      "21868460 2012-12-31 20:00:00         22.5   7.7        56  22.300123   \n",
      "21868461 2012-12-31 21:00:00         21.4   7.8        59  22.300123   \n",
      "21868462 2012-12-31 22:00:00         20.4   7.4        62  22.300123   \n",
      "21868463 2012-12-31 23:00:00         19.4   7.4        64  22.300123   \n",
      "\n",
      "                lon  \n",
      "0         76.412414  \n",
      "1         76.412414  \n",
      "2         76.412414  \n",
      "3         76.412414  \n",
      "4         76.412414  \n",
      "...             ...  \n",
      "21868459  73.198089  \n",
      "21868460  73.198089  \n",
      "21868461  73.198089  \n",
      "21868462  73.198089  \n",
      "21868463  73.198089  \n",
      "\n",
      "[21868464 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "df_dropped = all_weather_combined_pd.drop('year', axis=1) # or axis='columns'\n",
    "print(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bdc3e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df.to_csv('./error.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ed39137",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = pd.read_csv('./unique.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af813265",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk1 =  pd.read_csv('./missing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9adda4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.356045</td>\n",
       "      <td>78.455428</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.292231</td>\n",
       "      <td>75.567888</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.029740</td>\n",
       "      <td>79.186276</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.667798</td>\n",
       "      <td>83.364233</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.232530</td>\n",
       "      <td>81.232270</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latitude  Longitude  Year\n",
       "0  17.356045  78.455428  2015\n",
       "1  31.292231  75.567888  2012\n",
       "2  20.029740  79.186276  2015\n",
       "3  26.667798  83.364233  2012\n",
       "4  26.232530  81.232270  2012"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a694786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.400882</td>\n",
       "      <td>80.400005</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.667798</td>\n",
       "      <td>83.364233</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.926115</td>\n",
       "      <td>78.114098</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.232530</td>\n",
       "      <td>81.232270</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.292231</td>\n",
       "      <td>75.567888</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lat        lon  year\n",
       "0  26.400882  80.400005  2013\n",
       "1  26.667798  83.364233  2012\n",
       "2   9.926115  78.114098  2007\n",
       "3  26.232530  81.232270  2012\n",
       "4  31.292231  75.567888  2012"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95662ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in kk1 that are not present in kk:\n",
      "      Latitude  Longitude  Year\n",
      "89    9.001499  76.534752  2011\n",
      "245  27.484460  94.901945  2015\n",
      "246  32.250494  77.188138  2014\n",
      "249  18.500495  73.852904  2004\n",
      "250  22.347412  82.534030  2009\n",
      "..         ...        ...   ...\n",
      "465  22.230926  84.867870  2008\n",
      "466  23.631732  93.349318  2014\n",
      "467  28.657152  77.227513  2005\n",
      "468  28.657152  77.227513  2006\n",
      "469  17.330726  76.835254  2005\n",
      "\n",
      "[193 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Perform a left merge with an indicator\n",
    "merged_df = kk1.merge(\n",
    "    kk,\n",
    "    left_on=['Latitude', 'Longitude', 'Year'],\n",
    "    right_on=['lat', 'lon', 'year'],\n",
    "    how='left',\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# Filter for rows that only exist in kk1 (the left DataFrame)\n",
    "rows_only_in_kk1 = merged_df[merged_df['_merge'] == 'left_only']\n",
    "\n",
    "# Optional: Clean up the result to show only the columns from kk1\n",
    "rows_only_in_kk1 = rows_only_in_kk1[kk1.columns]\n",
    "\n",
    "print(\"Rows in kk1 that are not present in kk:\")\n",
    "print(rows_only_in_kk1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "929604ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = spark.read.parquet('./all_weather_pd_compatible.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d5a69c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+----+--------+---------+----------+\n",
      "|           datetime|temperature|wind|humidity|      lat|       lon|\n",
      "+-------------------+-----------+----+--------+---------+----------+\n",
      "|2013-01-01 00:00:00|       25.2| 7.1|      89|9.5006651|76.4124143|\n",
      "|2013-01-01 01:00:00|       25.1| 6.0|      89|9.5006651|76.4124143|\n",
      "|2013-01-01 02:00:00|       24.7| 4.7|      91|9.5006651|76.4124143|\n",
      "|2013-01-01 03:00:00|       24.3| 3.5|      93|9.5006651|76.4124143|\n",
      "|2013-01-01 04:00:00|       24.1| 4.6|      94|9.5006651|76.4124143|\n",
      "|2013-01-01 05:00:00|       23.9| 4.1|      94|9.5006651|76.4124143|\n",
      "|2013-01-01 06:00:00|       23.6| 3.5|      96|9.5006651|76.4124143|\n",
      "|2013-01-01 07:00:00|       24.2| 4.7|      95|9.5006651|76.4124143|\n",
      "|2013-01-01 08:00:00|       26.1| 5.4|      85|9.5006651|76.4124143|\n",
      "|2013-01-01 09:00:00|       27.2| 4.5|      79|9.5006651|76.4124143|\n",
      "|2013-01-01 10:00:00|       28.4| 2.6|      73|9.5006651|76.4124143|\n",
      "|2013-01-01 11:00:00|       29.4| 2.6|      67|9.5006651|76.4124143|\n",
      "|2013-01-01 12:00:00|       30.0| 3.4|      64|9.5006651|76.4124143|\n",
      "|2013-01-01 13:00:00|       30.4| 4.3|      62|9.5006651|76.4124143|\n",
      "|2013-01-01 14:00:00|       30.3| 6.2|      63|9.5006651|76.4124143|\n",
      "|2013-01-01 15:00:00|       30.1| 8.6|      64|9.5006651|76.4124143|\n",
      "|2013-01-01 16:00:00|       29.5| 8.6|      67|9.5006651|76.4124143|\n",
      "|2013-01-01 17:00:00|       28.9| 7.2|      71|9.5006651|76.4124143|\n",
      "|2013-01-01 18:00:00|       28.0| 4.6|      75|9.5006651|76.4124143|\n",
      "|2013-01-01 19:00:00|       27.4| 4.4|      80|9.5006651|76.4124143|\n",
      "+-------------------+-----------+----+--------+---------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "weather_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac11550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_only_in_kk1.to_csv('./rest_missing.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
