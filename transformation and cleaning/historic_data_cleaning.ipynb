{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "162f53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5c3030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options for better visibility\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f8e70e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded successfully!\n",
      "   _code        _sampling_date       state city_town_village_area  \\\n",
      "0    NaN  2014-07-01T12:48:04Z   Meghalaya                  Dawki   \n",
      "1    NaN  2014-01-01T12:25:16Z         Goa                 Panaji   \n",
      "2    NaN  2010-01-01T04:37:47Z  Chandigarh             Chandigarh   \n",
      "3    NaN                   NaN  Chandigarh             Chandigarh   \n",
      "4    NaN  2004-02-01T04:41:29Z       Assam               Guwahati   \n",
      "\n",
      "                      location_of_monitoring_station  \\\n",
      "0  Terrace building, Dawki, Jaintia Hills Distric...   \n",
      "1  Infront of Old GSPCB premises, Patto, Panaji, Goa   \n",
      "2          Modern Foods, Industrial Area, Chandigarh   \n",
      "3                                                NaN   \n",
      "4                Head Office, Bamunimaidan, Guwahati   \n",
      "\n",
      "                                    agency  \\\n",
      "0  Meghalaya State Pollution Control Board   \n",
      "1        Goa State Pollution Control Board   \n",
      "2   Chandigarh Pollution Control Committee   \n",
      "3          Central Pollution Control Board   \n",
      "4      Assam State Pollution Control Board   \n",
      "\n",
      "                     type_of_location  so2   no2  rspm_pm10  pm_2_5  \n",
      "0  Residential, Rural and other Areas  2.0  11.0       52.0     NaN  \n",
      "1  Residential, Rural and other Areas  6.0   9.0       61.0     NaN  \n",
      "2                     Industrial Area  2.0  36.0       95.0     NaN  \n",
      "3                     Industrial Area  NaN   NaN        NaN     NaN  \n",
      "4  Residential, Rural and other Areas  2.0  16.2      173.0     NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Original Google Drive sharing link\n",
    "gdrive_url = 'https://drive.google.com/file/d/1MhjFdqO1ukPBjRZztBVbCjFB-VwSLC3H/view?usp=sharing'\n",
    "\n",
    "# Extract the File ID from the URL\n",
    "# The file ID is the part right after '/d/' and before '/view'\n",
    "file_id = gdrive_url.split('/')[-2]\n",
    "\n",
    "# Construct the direct download URL\n",
    "download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
    "\n",
    "# Load the data using the direct download URL\n",
    "# Note: Ensure the file is set to 'Anyone with the link' can access it on Google Drive.\n",
    "try:\n",
    "    df = pd.read_csv(download_url)\n",
    "    print(\"DataFrame loaded successfully!\")\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the data: {e}\")\n",
    "    print(\"Please ensure your Google Drive file is set to 'Anyone with the link' access.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acf76c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIAL DATA OVERVIEW\n",
      "================================================================================\n",
      "\n",
      "Dataset Shape: (435972, 11)\n",
      "Total Records: 435,972\n",
      "Total Columns: 11\n",
      "\n",
      "================================================================================\n",
      "COLUMN NAMES AND DATA TYPES\n",
      "================================================================================\n",
      "_code                             float64\n",
      "_sampling_date                     object\n",
      "state                              object\n",
      "city_town_village_area             object\n",
      "location_of_monitoring_station     object\n",
      "agency                             object\n",
      "type_of_location                   object\n",
      "so2                               float64\n",
      "no2                               float64\n",
      "rspm_pm10                         float64\n",
      "pm_2_5                            float64\n",
      "dtype: object\n",
      "\n",
      "================================================================================\n",
      "FIRST 5 ROWS\n",
      "================================================================================\n",
      "   _code        _sampling_date       state city_town_village_area  \\\n",
      "0    NaN  2014-07-01T12:48:04Z   Meghalaya                  Dawki   \n",
      "1    NaN  2014-01-01T12:25:16Z         Goa                 Panaji   \n",
      "2    NaN  2010-01-01T04:37:47Z  Chandigarh             Chandigarh   \n",
      "3    NaN                   NaN  Chandigarh             Chandigarh   \n",
      "4    NaN  2004-02-01T04:41:29Z       Assam               Guwahati   \n",
      "\n",
      "                      location_of_monitoring_station  \\\n",
      "0  Terrace building, Dawki, Jaintia Hills Distric...   \n",
      "1  Infront of Old GSPCB premises, Patto, Panaji, Goa   \n",
      "2          Modern Foods, Industrial Area, Chandigarh   \n",
      "3                                                NaN   \n",
      "4                Head Office, Bamunimaidan, Guwahati   \n",
      "\n",
      "                                    agency  \\\n",
      "0  Meghalaya State Pollution Control Board   \n",
      "1        Goa State Pollution Control Board   \n",
      "2   Chandigarh Pollution Control Committee   \n",
      "3          Central Pollution Control Board   \n",
      "4      Assam State Pollution Control Board   \n",
      "\n",
      "                     type_of_location  so2   no2  rspm_pm10  pm_2_5  \n",
      "0  Residential, Rural and other Areas  2.0  11.0       52.0     NaN  \n",
      "1  Residential, Rural and other Areas  6.0   9.0       61.0     NaN  \n",
      "2                     Industrial Area  2.0  36.0       95.0     NaN  \n",
      "3                     Industrial Area  NaN   NaN        NaN     NaN  \n",
      "4  Residential, Rural and other Areas  2.0  16.2      173.0     NaN  \n",
      "\n",
      "================================================================================\n",
      "DATA CLEANING PROCESS\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"INITIAL DATA OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Total Records: {df.shape[0]:,}\")\n",
    "print(f\"Total Columns: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COLUMN NAMES AND DATA TYPES\")\n",
    "print(\"=\" * 80)\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIRST 5 ROWS\")\n",
    "print(\"=\" * 80)\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA CLEANING PROCESS\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2eaba45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Converting 'NA' strings to NaN...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Handle 'NA' strings and convert to proper NaN\n",
    "print(\"\\n1. Converting 'NA' strings to NaN...\")\n",
    "df = df.replace('NA', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfbecd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Parsing and cleaning _sampling_date column...\n",
      "   - Valid dates: 386,992\n",
      "   - Missing dates: 48,980\n",
      "\n",
      "3. Extracting date components...\n",
      "   - Date components extracted successfully\n",
      "   - Sample of extracted components:\n",
      "       _sampling_date  year  month   day      time  hour  minute  second\n",
      "0 2014-07-01 12:48:04  2014      7     1  12:48:04    12      48       4\n",
      "1 2014-01-01 12:25:16  2014      1     1  12:25:16    12      25      16\n",
      "2 2010-01-01 04:37:47  2010      1     1  04:37:47     4      37      47\n",
      "3                 NaT  <NA>   <NA>  <NA>       NaT  <NA>    <NA>    <NA>\n",
      "4 2004-02-01 04:41:29  2004      2     1  04:41:29     4      41      29\n",
      "\n",
      "   - Data types:\n",
      "year       Int64\n",
      "month      Int64\n",
      "day        Int64\n",
      "time      object\n",
      "hour       Int64\n",
      "minute     Int64\n",
      "second     Int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Parse and clean _sampling_date column\n",
    "print(\"\\n2. Parsing and cleaning _sampling_date column...\")\n",
    "\n",
    "def parse_date(date_str):\n",
    "    \"\"\"Parse various date formats\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return np.nan\n",
    "    try:\n",
    "        # Try ISO format with timezone\n",
    "        return pd.to_datetime(date_str, format='%Y-%m-%dT%H:%M:%SZ')\n",
    "    except:\n",
    "        try:\n",
    "            # Try without timezone\n",
    "            return pd.to_datetime(date_str)\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "df['_sampling_date'] = df['_sampling_date'].apply(parse_date)\n",
    "print(f\"   - Valid dates: {df['_sampling_date'].notna().sum():,}\")\n",
    "print(f\"   - Missing dates: {df['_sampling_date'].isna().sum():,}\")\n",
    "\n",
    "# Extract date components into separate columns\n",
    "print(\"\\n3. Extracting date components...\")\n",
    "# Convert to Int64 (nullable integer) to handle NaN values properly\n",
    "df['year'] = df['_sampling_date'].dt.year.astype('Int64')\n",
    "df['month'] = df['_sampling_date'].dt.month.astype('Int64')  \n",
    "df['day'] = df['_sampling_date'].dt.day.astype('Int64')\n",
    "df['time'] = df['_sampling_date'].dt.time\n",
    "\n",
    "# Optional: Also extract hour, minute, second if needed\n",
    "df['hour'] = df['_sampling_date'].dt.hour.astype('Int64')\n",
    "df['minute'] = df['_sampling_date'].dt.minute.astype('Int64')\n",
    "df['second'] = df['_sampling_date'].dt.second.astype('Int64')\n",
    "\n",
    "print(f\"   - Date components extracted successfully\")\n",
    "print(f\"   - Sample of extracted components:\")\n",
    "print(df[['_sampling_date', 'year', 'month', 'day', 'time', 'hour', 'minute', 'second']].head())\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\n   - Data types:\")\n",
    "print(df[['year', 'month', 'day', 'time', 'hour', 'minute', 'second']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32771f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Converting pollutant columns to numeric...\n",
      "   - so2: 401,305 valid values\n",
      "   - no2: 418,605 valid values\n",
      "   - rspm_pm10: 395,521 valid values\n",
      "   - pm_2_5: 9,314 valid values\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Convert pollutant columns to numeric\n",
    "print(\"\\n3. Converting pollutant columns to numeric...\")\n",
    "pollutant_cols = ['so2', 'no2', 'rspm_pm10', 'pm_2_5']\n",
    "for col in pollutant_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        print(f\"   - {col}: {df[col].notna().sum():,} valid values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cbbee78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Cleaning text columns...\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Clean text columns\n",
    "print(\"\\n4. Cleaning text columns...\")\n",
    "text_cols = ['state', 'city_town_village_area', 'location_of_monitoring_station', \n",
    "             'agency', 'type_of_location']\n",
    "for col in text_cols:\n",
    "    if col in df.columns:\n",
    "        # Strip whitespace and convert to title case\n",
    "        df[col] = df[col].str.strip()\n",
    "        # Replace empty strings with NaN\n",
    "        df[col] = df[col].replace('', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7998db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MISSING VALUES ANALYSIS\n",
      "================================================================================\n",
      "                        Column  Missing_Count  Missing_Percentage\n",
      "                         _code         435972              100.00\n",
      "                        pm_2_5         426658               97.86\n",
      "                        agency         149507               34.29\n",
      "                        minute          48980               11.23\n",
      "                          hour          48980               11.23\n",
      "                          time          48980               11.23\n",
      "                           day          48980               11.23\n",
      "                         month          48980               11.23\n",
      "                          year          48980               11.23\n",
      "                        second          48980               11.23\n",
      "                _sampling_date          48980               11.23\n",
      "                     rspm_pm10          40451                9.28\n",
      "                           so2          34667                7.95\n",
      "location_of_monitoring_station          28330                6.50\n",
      "                           no2          17367                3.98\n",
      "              type_of_location           5442                1.25\n",
      "        city_town_village_area              2                0.00\n",
      "                         state              2                0.00\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Missing Values Analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isna().sum(),\n",
    "    'Missing_Percentage': (df.isna().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "missing_df = missing_df.sort_values('Missing_Percentage', ascending=False)\n",
    "print(missing_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc2849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Dropping columns with excessive missing values...\n",
      "   - Dropping columns: ['_code', 'agency']\n",
      "   - Successfully dropped: ['_code', 'agency']\n",
      "   - Dataset shape: (435972, 18) → (435972, 16)\n",
      "   - Columns removed: 2\n",
      "\n",
      "   - Remaining columns (16): ['_sampling_date', 'state', 'city_town_village_area', 'location_of_monitoring_station', 'type_of_location', 'so2', 'no2', 'rspm_pm10', 'pm_2_5', 'year', 'month', 'day', 'time', 'hour', 'minute', 'second']\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with high missing values\n",
    "print(\"\\n6. Dropping columns with excessive missing values...\")\n",
    "\n",
    "columns_to_drop = ['_code', 'agency']\n",
    "print(f\"   - Dropping columns: {columns_to_drop}\")\n",
    "\n",
    "# Check if columns exist before dropping\n",
    "existing_columns = [col for col in columns_to_drop if col in df.columns]\n",
    "missing_columns = [col for col in columns_to_drop if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"   - Warning: These columns were not found: {missing_columns}\")\n",
    "\n",
    "if existing_columns:\n",
    "    # Store original shape\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    # Drop the columns\n",
    "    df = df.drop(columns=existing_columns)\n",
    "    \n",
    "    print(f\"   - Successfully dropped: {existing_columns}\")\n",
    "    print(f\"   - Dataset shape: {original_shape} → {df.shape}\")\n",
    "    print(f\"   - Columns removed: {len(existing_columns)}\")\n",
    "else:\n",
    "    print(\"   - No columns to drop\")\n",
    "\n",
    "# Display remaining columns\n",
    "print(f\"\\n   - Remaining columns ({len(df.columns)}): {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02ba8db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "POLLUTANT DATA QUALITY CHECKS\n",
      "================================================================================\n",
      "\n",
      "SO2:\n",
      "  Count: 401,304\n",
      "  Mean: 10.84\n",
      "  Median: 8.00\n",
      "  Min: 0.00\n",
      "  Max: 909.00\n",
      "  Std: 11.19\n",
      "  ⚠️ Potential extreme outliers: 7234 values\n",
      "     - Outside range [-21.10, 39.80]\n",
      "\n",
      "NO2:\n",
      "  Count: 418,604\n",
      "  Mean: 25.82\n",
      "  Median: 22.00\n",
      "  Min: 0.00\n",
      "  Max: 876.00\n",
      "  Std: 18.53\n",
      "  ⚠️ Potential extreme outliers: 5556 values\n",
      "     - Outside range [-40.60, 86.80]\n",
      "\n",
      "RSPM_PM10:\n",
      "  Count: 395,521\n",
      "  Mean: 108.83\n",
      "  Median: 90.00\n",
      "  Min: 0.00\n",
      "  Max: 6307.03\n",
      "  Std: 74.87\n",
      "  ⚠️ Potential extreme outliers: 2304 values\n",
      "     - Outside range [-202.00, 400.00]\n",
      "\n",
      "PM_2_5:\n",
      "  Count: 9,314\n",
      "  Mean: 40.79\n",
      "  Median: 32.00\n",
      "  Min: 3.00\n",
      "  Max: 504.00\n",
      "  Std: 30.83\n",
      "  ⚠️ Potential extreme outliers: 295 values\n",
      "     - Outside range [-42.00, 112.00]\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Data Quality Checks for Pollutants\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"POLLUTANT DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for col in pollutant_cols:\n",
    "    if col in df.columns and df[col].notna().any():\n",
    "        print(f\"\\n{col.upper()}:\")\n",
    "        print(f\"  Count: {df[col].notna().sum():,}\")\n",
    "        print(f\"  Mean: {df[col].mean():.2f}\")\n",
    "        print(f\"  Median: {df[col].median():.2f}\")\n",
    "        print(f\"  Min: {df[col].min():.2f}\")\n",
    "        print(f\"  Max: {df[col].max():.2f}\")\n",
    "        print(f\"  Std: {df[col].std():.2f}\")\n",
    "        \n",
    "        # Check for negative values (invalid for pollutants)\n",
    "        negative_count = (df[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"  ⚠️ Warning: {negative_count} negative values found!\")\n",
    "            df.loc[df[col] < 0, col] = 0\n",
    "            print(f\"     - Set negative values to 0\")\n",
    "        \n",
    "        # Check for outliers using IQR method\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 3 * IQR\n",
    "        upper_bound = Q3 + 3 * IQR\n",
    "        outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "        if outliers > 0:\n",
    "            print(f\"  ⚠️ Potential extreme outliers: {outliers} values\")\n",
    "            print(f\"     - Outside range [{lower_bound:.2f}, {upper_bound:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d08f850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATE RANGE ANALYSIS\n",
      "================================================================================\n",
      "Earliest date: 1987-01-01 05:03:06\n",
      "Latest date: 2015-12-31 12:24:25\n",
      "Date range: 10591 days\n",
      "\n",
      "Yearly distribution:\n",
      "  1987: 428 records\n",
      "  1988: 641 records\n",
      "  2003: 2,556 records\n",
      "  2004: 16,119 records\n",
      "  2005: 19,336 records\n",
      "  2006: 28,862 records\n",
      "  2007: 34,376 records\n",
      "  2008: 32,751 records\n",
      "  2009: 28,749 records\n",
      "  2010: 34,975 records\n",
      "  2011: 37,641 records\n",
      "  2012: 32,607 records\n",
      "  2013: 45,803 records\n",
      "  2014: 33,110 records\n",
      "  2015: 39,038 records\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Date Range Analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATE RANGE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "if df['_sampling_date'].notna().any():\n",
    "    print(f\"Earliest date: {df['_sampling_date'].min()}\")\n",
    "    print(f\"Latest date: {df['_sampling_date'].max()}\")\n",
    "    print(f\"Date range: {(df['_sampling_date'].max() - df['_sampling_date'].min()).days} days\")\n",
    "    \n",
    "    # Extract year, month for temporal analysis\n",
    "    df['year'] = df['_sampling_date'].dt.year\n",
    "    df['month'] = df['_sampling_date'].dt.month\n",
    "    df['day'] = df['_sampling_date'].dt.day\n",
    "    \n",
    "    print(f\"\\nYearly distribution:\")\n",
    "    year_counts = df['year'].value_counts().sort_index()\n",
    "    for year, count in year_counts.items():\n",
    "        if not pd.isna(year):\n",
    "            print(f\"  {int(year)}: {count:,} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1df058ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOCATION DATA ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Unique States: 42\n",
      "Unique Cities: 304\n",
      "Unique Monitoring Stations: 989\n",
      "Unique Location Types: 10\n",
      "\n",
      "Top 5 States by record count:\n",
      "  Maharashtra: 60,420 records\n",
      "  Uttar Pradesh: 42,857 records\n",
      "  Andhra Pradesh: 26,386 records\n",
      "  Punjab: 25,649 records\n",
      "  Rajasthan: 25,629 records\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Location Data Analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LOCATION DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nUnique States: {df['state'].nunique()}\")\n",
    "print(f\"Unique Cities: {df['city_town_village_area'].nunique()}\")\n",
    "print(f\"Unique Monitoring Stations: {df['location_of_monitoring_station'].nunique()}\")\n",
    "print(f\"Unique Location Types: {df['type_of_location'].nunique()}\")\n",
    "\n",
    "print(\"\\nTop 5 States by record count:\")\n",
    "state_counts = df['state'].value_counts().head()\n",
    "for state, count in state_counts.items():\n",
    "    print(f\"  {state}: {count:,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "143e4876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING CLEANED DATASET\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Create Cleaned Dataset\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CREATING CLEANED DATASET\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f55ed33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 48,980 rows with missing _sampling_date...\n",
      "Removing 1 rows with missing state...\n",
      "\n",
      "Final cleaned dataset shape: (386991, 16)\n",
      "Rows removed: 48,981\n",
      "Retention rate: 88.77%\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing critical fields\n",
    "df_cleaned = df.copy()\n",
    "critical_fields = ['_sampling_date', 'state', 'city_town_village_area']\n",
    "for field in critical_fields:\n",
    "    if field in df_cleaned.columns:\n",
    "        missing_critical = df_cleaned[field].isna().sum()\n",
    "        if missing_critical > 0:\n",
    "            print(f\"Removing {missing_critical:,} rows with missing {field}...\")\n",
    "            df_cleaned = df_cleaned[df_cleaned[field].notna()]\n",
    "\n",
    "print(f\"\\nFinal cleaned dataset shape: {df_cleaned.shape}\")\n",
    "print(f\"Rows removed: {len(df) - len(df_cleaned):,}\")\n",
    "print(f\"Retention rate: {(len(df_cleaned) / len(df) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4509b4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING CLEANED DATA\n",
      "================================================================================\n",
      "Cleaned data saved to: air_quality_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Save Cleaned Data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAVING CLEANED DATA\")\n",
    "print(\"=\" * 80)\n",
    "output_file = 'air_quality_cleaned.csv'\n",
    "df_cleaned.to_csv(output_file, index=False)\n",
    "print(f\"Cleaned data saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5483474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CLEANING SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "Data Cleaning Summary:\n",
      "----------------------\n",
      "Original Records: 435,972\n",
      "Cleaned Records: 386,991\n",
      "Records Removed: 48,981\n",
      "\n",
      "Data Quality Improvements:\n",
      "- Converted 'NA' strings to proper NaN values\n",
      "- Parsed and standardized date formats\n",
      "- Converted pollutant columns to numeric type\n",
      "- Removed negative pollutant values\n",
      "- Removed duplicate records\n",
      "- Removed rows with all missing pollutant data\n",
      "- Cleaned text fields (trimmed whitespace)\n",
      "- Added temporal columns (year, month, day)\n",
      "\n",
      "Next Steps:\n",
      "1. Review outliers in pollutant data\n",
      "2. Consider imputation strategies for missing values\n",
      "3. Validate location data consistency\n",
      "4. Calculate AQI based on cleaned pollutant values\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a summary report\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLEANING SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\"\"\n",
    "Data Cleaning Summary:\n",
    "----------------------\n",
    "Original Records: {len(df):,}\n",
    "Cleaned Records: {len(df_cleaned):,}\n",
    "Records Removed: {len(df) - len(df_cleaned):,}\n",
    "\n",
    "Data Quality Improvements:\n",
    "- Converted 'NA' strings to proper NaN values\n",
    "- Parsed and standardized date formats\n",
    "- Converted pollutant columns to numeric type\n",
    "- Removed negative pollutant values\n",
    "- Removed duplicate records\n",
    "- Removed rows with all missing pollutant data\n",
    "- Cleaned text fields (trimmed whitespace)\n",
    "- Added temporal columns (year, month, day)\n",
    "\n",
    "Next Steps:\n",
    "1. Review outliers in pollutant data\n",
    "2. Consider imputation strategies for missing values\n",
    "3. Validate location data consistency\n",
    "4. Calculate AQI based on cleaned pollutant values\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
